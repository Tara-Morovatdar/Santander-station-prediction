{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from pandas import read_csv\n",
    "import numpy as np # linear algebra\n",
    "import random as rd # generating random numbers\n",
    "import datetime # manipulating date formats\n",
    "import time\n",
    "import ast\n",
    "from math import sqrt\n",
    "# Viz\n",
    "import seaborn as sns # for prettier plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# TIME SERIES\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import seaborn; seaborn.set()\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/bigdata/tara/ind_london_2018_sample.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ucl_id</th>\n",
       "      <th>operator_intid</th>\n",
       "      <th>operator_altid</th>\n",
       "      <th>operator_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>initial_bikes</th>\n",
       "      <th>initial_size</th>\n",
       "      <th>curr_bikes</th>\n",
       "      <th>curr_size</th>\n",
       "      <th>created_dt</th>\n",
       "      <th>updated_dt</th>\n",
       "      <th>neighbors_1</th>\n",
       "      <th>neighbors_2</th>\n",
       "      <th>station_id</th>\n",
       "      <th>change_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Malet Street, Bloomsbury</td>\n",
       "      <td>51.521681</td>\n",
       "      <td>-0.130432</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[364, 287]</td>\n",
       "      <td>[364, 287, 88, 19, 796]</td>\n",
       "      <td>12</td>\n",
       "      <td>38553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>Little Argyll Street, West End</td>\n",
       "      <td>51.514500</td>\n",
       "      <td>-0.141424</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[349, 159]</td>\n",
       "      <td>[159, 349, 313, 106, 141]</td>\n",
       "      <td>116</td>\n",
       "      <td>37611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>Leonard Circus , Shoreditch</td>\n",
       "      <td>51.524696</td>\n",
       "      <td>-0.084439</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[323]</td>\n",
       "      <td>[323, 73, 58, 3, 319]</td>\n",
       "      <td>32</td>\n",
       "      <td>40529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>Milroy Walk, South Bank</td>\n",
       "      <td>51.507244</td>\n",
       "      <td>-0.106238</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[839, 230, 240, 792]</td>\n",
       "      <td>[839, 240, 230, 792, 420]</td>\n",
       "      <td>195</td>\n",
       "      <td>36805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>Holborn Circus, Holborn</td>\n",
       "      <td>51.517950</td>\n",
       "      <td>-0.108657</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[546, 67, 835]</td>\n",
       "      <td>[546, 67, 835, 84, 112]</td>\n",
       "      <td>66</td>\n",
       "      <td>49363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ucl_id  operator_intid  operator_altid                   operator_name  \\\n",
       "0      12              12              12        Malet Street, Bloomsbury   \n",
       "1     116             116             116  Little Argyll Street, West End   \n",
       "3      32              32              32     Leonard Circus , Shoreditch   \n",
       "6     195             195             195         Milroy Walk, South Bank   \n",
       "7      66              66              66         Holborn Circus, Holborn   \n",
       "\n",
       "         lat       lon  initial_bikes  initial_size  curr_bikes  curr_size  \\\n",
       "0  51.521681 -0.130432             25            40          26         49   \n",
       "1  51.514500 -0.141424             12            21           5         21   \n",
       "3  51.524696 -0.084439             17            21           9         43   \n",
       "6  51.507244 -0.106238              3            30          19         30   \n",
       "7  51.517950 -0.108657             39            39          26         40   \n",
       "\n",
       "            created_dt           updated_dt           neighbors_1  \\\n",
       "0  2010-08-06 01:00:00  2020-02-14 17:38:02            [364, 287]   \n",
       "1  2010-08-06 01:00:00  2020-02-14 17:38:02            [349, 159]   \n",
       "3  2010-08-06 01:00:00  2020-02-14 17:38:02                 [323]   \n",
       "6  2010-08-06 01:00:00  2020-02-14 17:38:02  [839, 230, 240, 792]   \n",
       "7  2010-08-06 01:00:00  2020-02-14 17:38:02        [546, 67, 835]   \n",
       "\n",
       "                 neighbors_2  station_id  change_count  \n",
       "0    [364, 287, 88, 19, 796]          12         38553  \n",
       "1  [159, 349, 313, 106, 141]         116         37611  \n",
       "3      [323, 73, 58, 3, 319]          32         40529  \n",
       "6  [839, 240, 230, 792, 420]         195         36805  \n",
       "7    [546, 67, 835, 84, 112]          66         49363  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station=pd.read_csv('SampleData/sample_stations.csv',index_col=0)\n",
    "df_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12, 116,  32, 195,  66, 194,  17, 270, 130, 278, 101, 579, 703,\n",
       "       109, 564,  73,  71, 228, 331, 251, 427, 229, 217, 177,  95, 246,\n",
       "        64, 340, 193,  14, 272, 192,  55, 107,  48, 199, 159, 541, 154,\n",
       "       338, 213, 341, 732,  18, 214, 374, 361])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station.ucl_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 47 entries, 0 to 65\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   ucl_id          47 non-null     int64  \n",
      " 1   operator_intid  47 non-null     int64  \n",
      " 2   operator_altid  47 non-null     int64  \n",
      " 3   operator_name   47 non-null     object \n",
      " 4   lat             47 non-null     float64\n",
      " 5   lon             47 non-null     float64\n",
      " 6   initial_bikes   47 non-null     int64  \n",
      " 7   initial_size    47 non-null     int64  \n",
      " 8   curr_bikes      47 non-null     int64  \n",
      " 9   curr_size       47 non-null     int64  \n",
      " 10  created_dt      47 non-null     object \n",
      " 11  updated_dt      47 non-null     object \n",
      " 12  neighbors_1     47 non-null     object \n",
      " 13  neighbors_2     47 non-null     object \n",
      " 14  station_id      47 non-null     int64  \n",
      " 15  change_count    47 non-null     int64  \n",
      "dtypes: float64(2), int64(9), object(5)\n",
      "memory usage: 6.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_station.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates=['2018-01-01','2018-02-01','2018-03-01','2018-04-01','2018-05-01','2018-06-01','2018-07-01','2018-08-01',\n",
    "             '2018-09-01','2018-10-01','2018-11-01','2018-12-01']\n",
    "end_dates=['2018-01-31','2018-02-28','2018-03-31','2018-04-30','2018-05-31','2018-06-30','2018-07-31','2018-08-31'\n",
    "           ,'2018-09-30','2018-10-31','2018-11-30','2018-12-31']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq=60 #the original freq of data is 2 minutes interval\n",
    "time_offset=10 #for how many minutes ahead we want to predict\n",
    "#forecast_steps=int(time_offset/freq) #number of predictions that we need to predict time_offset ahead\n",
    "test_days=7 #the number of days for test\n",
    "test_size=int(test_days*24*60/freq) \n",
    "forecast_steps=int(time_offset/freq)\n",
    "# window_size=int(24*60/freq)\n",
    "window_size=int(4*60/freq)\n",
    "Stations=list(df_station['ucl_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtest_size=int((test_days-1)*24*60/freq)\n",
    "#xtest_size=984\n",
    "\n",
    "#xtest_size=int((test_days-1)*24*60/freq)\n",
    "xtest_size=164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_seasonality(temp_df,test_size):\n",
    "    df_temp=temp_df.copy()\n",
    "    avgs=df_temp[0:-test_size].groupby([df_temp[0:-test_size].index.dayofweek,df_temp[0:-test_size].index.time]).mean()\n",
    "    for name, column in (df_temp.iteritems()):\n",
    "        df_temp[name+'_avg'] = df_temp.index.map(lambda d: avgs.loc[(d.dayofweek,d.time()),name])\n",
    "        df_temp[name] = df_temp[name] - df_temp[name+'_avg']\n",
    "    \n",
    "        #df_temp=df_temp.drop(name+'_avg',axis=1)\n",
    "    n_cols=len(list(df_temp.columns))\n",
    "    df1 = df_temp.iloc[:, :int(n_cols/2)] #var_df\n",
    "    df2 = df_temp.iloc[:, int(n_cols/2):] #vard_df_avg\n",
    "    return df1,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_seasonality(df1,df2):\n",
    "    df_1=df1.copy()\n",
    "    df_2=df2.copy()\n",
    "    for name, column in (df_1.iteritems()):\n",
    "        \n",
    "        df_1[name] = df_1[name] + df_2[name.split('_forecast_')[0]+'_avg']\n",
    "    return df_1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarima(p,d,q,station_id):\n",
    "    global i\n",
    "#     global model_times\n",
    "      \n",
    "    df_history=df_train.copy()\n",
    "    forecasts=[]\n",
    "    tests=[]\n",
    "    # walk-forward validation\n",
    "    \n",
    "    for t in range(0,len(df_test),window_size):\n",
    "        result={}\n",
    "        predictions = list()\n",
    "        #because of the warning for frequency\n",
    "        df_history.index = pd.DatetimeIndex(df_history.index.values,\n",
    "                                   freq=df_history.index.inferred_freq)\n",
    "        # fit model\n",
    "        start=time.time()\n",
    "        model = SARIMAX(df_history[-len(df_train):],order=(p,d,q),seasonal_order=(0,0,0,0),enforce_stationarity=False, enforce_invertibility=False)\n",
    "        model_fit = model.fit()\n",
    "\n",
    "\n",
    "        yhat = model_fit.forecast(steps=window_size)\n",
    "        end=time.time()\n",
    "        model_times.append(end-start)\n",
    "        # store forecast and ob\n",
    "        \n",
    "        df_history = df_history.append(df_test[t:t+window_size])\n",
    "        \n",
    "        #df_forecast = pd.DataFrame(yhat, index=df_test.index[t:t+window_size], columns=df_train.columns + '_forecast_'+str(d))\n",
    "        predictions.extend(yhat)\n",
    "\n",
    "        if t==xtest_size:\n",
    "            df_forecast = pd.DataFrame(predictions, index=data.index[-test_size+t:], columns=data.columns + '_forecast_'+str(d))\n",
    "        else:\n",
    "            df_forecast = pd.DataFrame(predictions, index=data.index[-test_size+t:-test_size+t+window_size], columns=data.columns + '_forecast_'+str(d))\n",
    "        \n",
    "        #######bring forecast data back to normal scale \n",
    "       \n",
    "        df_forecast= add_seasonality(df_forecast,df_test_avg[t:t+window_size])  \n",
    "    \n",
    "        df_forecast=utils.inverse_normalize(df_forecast,total_dock)\n",
    "        \n",
    "        ####bring test data back to normal scale \n",
    "        df_actual= add_seasonality(df_test[t:t+window_size],df_test_avg[t:t+window_size])       \n",
    " \n",
    "        df_actual=utils.inverse_normalize(df_actual,total_dock)       \n",
    "\n",
    "        \n",
    "        df_forecast= utils.forecast_truncate(df_forecast,total_dock)\n",
    "        \n",
    "        ######################write to file\n",
    "        forecasts.extend(list(df_forecast.iloc[:,0].values))\n",
    "        tests.extend(list(df_actual.iloc[:,0].values))\n",
    "#     plt.figure(figsize=(15,5))\n",
    "#     plt.plot(original_data.iloc[0:len(original_data)-test_size,0].values)\n",
    "#     plt.plot([None for i in df_train.iloc[:,0].values] + [x for x in tests])\n",
    "#     plt.plot([None for i in df_train.iloc[:,0].values] + [x for x in forecasts])\n",
    "\n",
    "\n",
    "        if ((t+window_size)%(6*window_size) == 0):\n",
    "                result['day']=df_forecast.index[1].dayofweek\n",
    "                result['month']=df_forecast.index[1].month\n",
    "                result['station_id']=station_id\n",
    "    #             result['predict']=list(df_forecast.iloc[:,0].values)\n",
    "    #             result['test']=list(df_actual.iloc[:,0].values)\n",
    "                result['predict']=forecasts\n",
    "                result['test']=tests\n",
    "\n",
    "                with open('report/test/regressors/SARIMA/'+'sarima_result_'+str(station_id)+'_'+str(i)+'.json', 'w+') as f:\n",
    "                #with open('report/SARIMA/'+str(freq)+'minutes/sarima_result_'+str(station_id)+'_'+str(i)+'.json', 'w+') as f:\n",
    "                    f.write(json.dumps(result))\n",
    "                i=i+1\n",
    "                forecasts=[]\n",
    "                tests=[]\n",
    "\n",
    "\n",
    " \n",
    "    #return df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01\n",
      "1\n",
      "2018-02-01\n",
      "1\n",
      "1\n",
      "2018-03-01\n",
      "1\n",
      "1\n",
      "2018-04-01\n",
      "1\n",
      "2018-05-01\n",
      "1\n",
      "2018-06-01\n",
      "1\n",
      "2018-07-01\n",
      "2018-08-01\n",
      "2018-09-01\n",
      "1\n",
      "2018-10-01\n",
      "1\n",
      "2018-11-01\n",
      "1\n",
      "2018-12-01\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "sarima_report=[]\n",
    "# p_orders=list()\n",
    "# p_order_times=list()\n",
    "model_times=list()\n",
    "i=0\n",
    "for t in range(0,12):\n",
    "  \n",
    "    start_date=start_dates[t]\n",
    "    end_date=end_dates[t]        \n",
    "    print(start_date)\n",
    "    for station_id in Stations:\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_dock=df[df['operator_id']==int(station_id)]['total_docks'].values[0]\n",
    "        data=df[df['operator_id']==station_id]\n",
    "        #data prepration       \n",
    "        \n",
    "        data=utils.clean_df(data,station_id,start_date,end_date,freq)\n",
    "        original_data=data\n",
    "        data=utils.normalize(data,total_dock)\n",
    "        anomalies=utils.anomaly_detection(data,freq)\n",
    "        data=utils.anomaly_removal(anomalies,data)\n",
    "        #checking for stationarity\n",
    "        data,data_avg=remove_seasonality(data,test_size)\n",
    "        a,d=utils.make_Stationary(data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df_train, df_test = data[0:len(data)-test_size], data[len(data)-test_size:]\n",
    "        df_train_avg, df_test_avg = data_avg[0:len(data)-test_size], data_avg[len(data)-test_size:]\n",
    "        \n",
    "#         start=time.time()\n",
    "#         res = sm.tsa.arma_order_select_ic(df_train, max_ar=12, max_ma=0, ic=['bic'])#,trend='c'\n",
    "#         end=time.time()\n",
    "        \n",
    "#         p_order_times.append(end-start)\n",
    "        \n",
    "#         p=res.bic_min_order[0]\n",
    "#         p_orders.append(p)\n",
    "        if d>0:\n",
    "            print(d)\n",
    "        #apply the model and get the result\n",
    "        Sarima(2,d,0,station_id)\n",
    "#         break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sarima_model_time.txt', 'w+') as f:\n",
    "    for item in model_times:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
