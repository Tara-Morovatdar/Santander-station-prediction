{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from pandas import read_csv\n",
    "import numpy as np # linear algebra\n",
    "import random as rd # generating random numbers\n",
    "import datetime # manipulating date formats\n",
    "import time\n",
    "import ast\n",
    "from math import sqrt\n",
    "# Viz\n",
    "import seaborn as sns # for prettier plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# TIME SERIES\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import seaborn; seaborn.set()\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/bigdata/tara/ind_london_2018_sample.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ucl_id</th>\n",
       "      <th>operator_intid</th>\n",
       "      <th>operator_altid</th>\n",
       "      <th>operator_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>initial_bikes</th>\n",
       "      <th>initial_size</th>\n",
       "      <th>curr_bikes</th>\n",
       "      <th>curr_size</th>\n",
       "      <th>created_dt</th>\n",
       "      <th>updated_dt</th>\n",
       "      <th>neighbors_1</th>\n",
       "      <th>neighbors_2</th>\n",
       "      <th>station_id</th>\n",
       "      <th>change_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Malet Street, Bloomsbury</td>\n",
       "      <td>51.521681</td>\n",
       "      <td>-0.130432</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[364, 287]</td>\n",
       "      <td>[364, 287, 88, 19, 796]</td>\n",
       "      <td>12</td>\n",
       "      <td>38553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>Little Argyll Street, West End</td>\n",
       "      <td>51.514500</td>\n",
       "      <td>-0.141424</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[349, 159]</td>\n",
       "      <td>[159, 349, 313, 106, 141]</td>\n",
       "      <td>116</td>\n",
       "      <td>37611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>Leonard Circus , Shoreditch</td>\n",
       "      <td>51.524696</td>\n",
       "      <td>-0.084439</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[323]</td>\n",
       "      <td>[323, 73, 58, 3, 319]</td>\n",
       "      <td>32</td>\n",
       "      <td>40529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>Milroy Walk, South Bank</td>\n",
       "      <td>51.507244</td>\n",
       "      <td>-0.106238</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[839, 230, 240, 792]</td>\n",
       "      <td>[839, 240, 230, 792, 420]</td>\n",
       "      <td>195</td>\n",
       "      <td>36805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>Holborn Circus, Holborn</td>\n",
       "      <td>51.517950</td>\n",
       "      <td>-0.108657</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[546, 67, 835]</td>\n",
       "      <td>[546, 67, 835, 84, 112]</td>\n",
       "      <td>66</td>\n",
       "      <td>49363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ucl_id  operator_intid  operator_altid                   operator_name  \\\n",
       "0      12              12              12        Malet Street, Bloomsbury   \n",
       "1     116             116             116  Little Argyll Street, West End   \n",
       "3      32              32              32     Leonard Circus , Shoreditch   \n",
       "6     195             195             195         Milroy Walk, South Bank   \n",
       "7      66              66              66         Holborn Circus, Holborn   \n",
       "\n",
       "         lat       lon  initial_bikes  initial_size  curr_bikes  curr_size  \\\n",
       "0  51.521681 -0.130432             25            40          26         49   \n",
       "1  51.514500 -0.141424             12            21           5         21   \n",
       "3  51.524696 -0.084439             17            21           9         43   \n",
       "6  51.507244 -0.106238              3            30          19         30   \n",
       "7  51.517950 -0.108657             39            39          26         40   \n",
       "\n",
       "            created_dt           updated_dt           neighbors_1  \\\n",
       "0  2010-08-06 01:00:00  2020-02-14 17:38:02            [364, 287]   \n",
       "1  2010-08-06 01:00:00  2020-02-14 17:38:02            [349, 159]   \n",
       "3  2010-08-06 01:00:00  2020-02-14 17:38:02                 [323]   \n",
       "6  2010-08-06 01:00:00  2020-02-14 17:38:02  [839, 230, 240, 792]   \n",
       "7  2010-08-06 01:00:00  2020-02-14 17:38:02        [546, 67, 835]   \n",
       "\n",
       "                 neighbors_2  station_id  change_count  \n",
       "0    [364, 287, 88, 19, 796]          12         38553  \n",
       "1  [159, 349, 313, 106, 141]         116         37611  \n",
       "3      [323, 73, 58, 3, 319]          32         40529  \n",
       "6  [839, 240, 230, 792, 420]         195         36805  \n",
       "7    [546, 67, 835, 84, 112]          66         49363  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station=pd.read_csv('SampleData/sample_stations.csv',index_col=0)\n",
    "df_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>trip_count</th>\n",
       "      <th>temp</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>humidity</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>Clear</th>\n",
       "      <th>Clouds</th>\n",
       "      <th>...</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Thunderstorm</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>trip_log</th>\n",
       "      <th>is_non_workday</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_rushhour</th>\n",
       "      <th>is_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>754</td>\n",
       "      <td>280.27</td>\n",
       "      <td>996</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.626718</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02 00:00:00</td>\n",
       "      <td>90</td>\n",
       "      <td>279.16</td>\n",
       "      <td>1009</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.510860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03 00:00:00</td>\n",
       "      <td>70</td>\n",
       "      <td>285.27</td>\n",
       "      <td>988</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>76</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.262680</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04 00:00:00</td>\n",
       "      <td>121</td>\n",
       "      <td>280.77</td>\n",
       "      <td>1001</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.804021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>94</td>\n",
       "      <td>278.85</td>\n",
       "      <td>993</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>81</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.553877</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  trip_count    temp  pressure  wind_speed  rain_1h  \\\n",
       "0  2018-01-01 00:00:00         754  280.27       996           9      0.0   \n",
       "1  2018-01-02 00:00:00          90  279.16      1009           4      0.0   \n",
       "2  2018-01-03 00:00:00          70  285.27       988          11      3.0   \n",
       "3  2018-01-04 00:00:00         121  280.77      1001           6      0.0   \n",
       "4  2018-01-05 00:00:00          94  278.85       993           3      0.3   \n",
       "\n",
       "   humidity  clouds_all  Clear  Clouds  ...  Smoke  Snow  Thunderstorm  \\\n",
       "0        70          68      0       1  ...      0     0             0   \n",
       "1        75          48      0       1  ...      0     0             0   \n",
       "2        76          92      0       0  ...      0     0             0   \n",
       "3        75          40      0       1  ...      0     0             0   \n",
       "4        81          92      0       0  ...      0     0             0   \n",
       "\n",
       "   is_weekend  is_holiday  trip_log  is_non_workday  hour  is_rushhour  is_day  \n",
       "0           0           1  6.626718               1     0            0       0  \n",
       "1           0           0  4.510860               0     0            0       0  \n",
       "2           0           0  4.262680               0     0            0       0  \n",
       "3           0           0  4.804021               0     0            0       0  \n",
       "4           0           0  4.553877               0     0            0       0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exg=pd.read_csv('SampleData/exg_df.csv',index_col=0)\n",
    "\n",
    "df_exg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates=['2018-01-01','2018-02-01','2018-03-01','2018-04-01','2018-05-01','2018-06-01','2018-07-01','2018-08-01',\n",
    "             '2018-09-01','2018-10-01','2018-11-01','2018-12-01']\n",
    "end_dates=['2018-01-31','2018-02-28','2018-03-31','2018-04-30','2018-05-31','2018-06-30','2018-07-31','2018-08-31'\n",
    "           ,'2018-09-30','2018-10-31','2018-11-30','2018-12-31']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq=60 #the original freq of data is 2 minutes interval\n",
    "time_offset=60 #for how many minutes ahead we want to predict\n",
    "#forecast_steps=int(time_offset/freq) #number of predictions that we need to predict time_offset ahead\n",
    "test_days=7 #the number of days for test\n",
    "test_size=int(test_days*24*60/freq) \n",
    "forecast_steps=int(time_offset/freq)\n",
    "window_size=int(4*60/freq)\n",
    "Stations=list(df_station['ucl_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_size=164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_seasonality(temp_df,test_size):\n",
    "    df_temp=temp_df.copy()\n",
    "    avgs=df_temp[0:-test_size].groupby([df_temp[0:-test_size].index.dayofweek,df_temp[0:-test_size].index.time]).mean()\n",
    "    for name, column in (df_temp.iteritems()):\n",
    "        df_temp[name+'_avg'] = df_temp.index.map(lambda d: avgs.loc[(d.dayofweek,d.time()),name])\n",
    "        df_temp[name] = df_temp[name] - df_temp[name+'_avg']\n",
    "    \n",
    "        #df_temp=df_temp.drop(name+'_avg',axis=1)\n",
    "    n_cols=len(list(df_temp.columns))\n",
    "    df1 = df_temp.iloc[:, :int(n_cols/2)] #var_df\n",
    "    df2 = df_temp.iloc[:, int(n_cols/2):] #vard_df_avg\n",
    "    return df1,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_seasonality(df1,df2):\n",
    "    df_1=df1.copy()\n",
    "    df_2=df2.copy()\n",
    "    for name, column in (df_1.iteritems()):\n",
    "        \n",
    "        df_1[name] = df_1[name] + df_2[name.split('_forecast_')[0]+'_avg']\n",
    "    return df_1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarima(p,d,q,station_id):\n",
    "    global i\n",
    "    global summaries\n",
    "    global model_times\n",
    "    forecasts=[]\n",
    "    tests=[]\n",
    "    df_exg_history= data_exg_train.copy()\n",
    "    df_history=df_train.copy()\n",
    "    t0=test_size ##neded for reverse the diff\n",
    "    \n",
    "    t0=test_size ##neded for reverse the diff  \n",
    "    \n",
    "    # walk-forward validation\n",
    "    \n",
    "    \n",
    "    for t in range(0,len(df_test),window_size):\n",
    "        result={}\n",
    "        predictions = list()\n",
    "        #because of the warning for frequency\n",
    "        df_history.index = pd.DatetimeIndex(df_history.index.values,\n",
    "                                   freq=df_history.index.inferred_freq)\n",
    "        # fit model\n",
    "        start=time.time()\n",
    "        model = SARIMAX(df_history[-len(df_train):],order=(p,d,q),seasonal_order=(0,0,0,0),\n",
    "                        enforce_stationarity=False, enforce_invertibility=False,\n",
    "                        exog=df_exg_history[-len(df_train):], time_varying_regression=True, mle_regression=False)\n",
    "\n",
    "        model_fit = model.fit()\n",
    "        end=time.time()\n",
    "        model_times.append(end-start)\n",
    "#         print(model_fit.summary())\n",
    "        results_summary=model_fit.summary()\n",
    "        results_as_html = results_summary.tables[1].as_html()\n",
    "        df_summary=pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "        summaries.append(df_summary)\n",
    "        #print(df_summary)\n",
    "        yhat = model_fit.forecast(steps=window_size,exog =data_exg_test[t:t+window_size])\n",
    "     \n",
    "        ### update indogenous and exogenous dataframes\n",
    "        df_history = df_history.append(df_test[t:t+window_size])\n",
    "        df_exg_history=df_exg_history.append(data_exg_test[t:t+window_size])\n",
    "        \n",
    "        #df_forecast = pd.DataFrame(yhat, index=df_test.index[t:t+window_size], columns=df_train.columns + '_forecast_'+str(d))\n",
    "        predictions.extend(yhat)\n",
    "\n",
    "        if t==xtest_size:\n",
    "            df_forecast = pd.DataFrame(predictions, index=data.index[-test_size+t:], columns=data.columns + '_forecast_'+str(d))\n",
    "        else:\n",
    "            df_forecast = pd.DataFrame(predictions, index=data.index[-test_size+t:-test_size+t+window_size], \n",
    "                                       columns=data.columns + '_forecast_'+str(d))\n",
    "        \n",
    "        #######bring forecast data back to normal scale \n",
    "       \n",
    "        df_forecast= add_seasonality(df_forecast,df_test_avg[t:t+window_size])  \n",
    "        #df_forecast=utils.invert_transformation_forecast(data_original, df_forecast,t0,diff_count=d)\n",
    "        df_forecast=utils.inverse_normalize(df_forecast,total_dock)\n",
    "        \n",
    "        ####bring test data back to normal scale \n",
    "        df_actual= add_seasonality(df_test[t:t+window_size],df_test_avg[t:t+window_size])\n",
    "       \n",
    "        #df_actual= utils.invert_transformation_test(data_original, df_test,t0,diff_count=d)\n",
    "        df_actual=utils.inverse_normalize(df_actual,total_dock) \n",
    "        \n",
    "       \n",
    " \n",
    "        t0=test_size +window_size\n",
    "        \n",
    "        df_forecast= utils.forecast_truncate(df_forecast,total_dock)\n",
    "        ######################write to file\n",
    "        forecasts.extend(list(df_forecast.iloc[:,0].values))\n",
    "        tests.extend(list(df_actual.iloc[:,0].values))\n",
    "\n",
    "        if ((t+window_size)%(6*window_size) == 0):\n",
    "                result['day']=df_forecast.index[1].dayofweek\n",
    "                result['month']=df_forecast.index[1].month\n",
    "                result['station_id']=station_id\n",
    "    #             result['predict']=list(df_forecast.iloc[:,0].values)\n",
    "    #             result['test']=list(df_actual.iloc[:,0].values)\n",
    "                result['predict']=forecasts\n",
    "                result['test']=tests\n",
    "\n",
    "                with open('report/test/regressors/SARIMAX0/'+'sarima_result_'+str(station_id)+'_'+str(i)+'.json', 'w+') as f:\n",
    "                #with open('report/SARIMA/'+str(freq)+'minutes/sarima_result_'+str(station_id)+'_'+str(i)+'.json', 'w+') as f:\n",
    "                    f.write(json.dumps(result))\n",
    "                i=i+1\n",
    "                forecasts=[]\n",
    "                tests=[]\n",
    "#         break\n",
    "    #return df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "summaries=[]\n",
    "model_times=list()\n",
    "i=0\n",
    "for t in range(0,1):\n",
    "  \n",
    "    start_date=start_dates[t]\n",
    "    end_date=end_dates[t]        \n",
    "    print(start_date)\n",
    "    data_exg_original=utils.clean_exg_df(df_exg,start_date,end_date,freq)\n",
    "    for station_id in Stations:\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_dock=df[df['operator_id']==int(station_id)]['total_docks'].values[0]\n",
    "        data=df[df['operator_id']==station_id]\n",
    "        #data prepration       \n",
    "        \n",
    "        data=utils.clean_df(data,station_id,start_date,end_date,freq)\n",
    "        \n",
    "        data_exg=data_exg_original[data_exg_original.index.isin(data.index)]\n",
    "        data=utils.normalize(data,total_dock)\n",
    "        anomalies=utils.anomaly_detection(data,freq)\n",
    "        data=utils.anomaly_removal(anomalies,data)\n",
    "        \n",
    "        data,data_avg=remove_seasonality(data,test_size)\n",
    "        #checking for stationarity\n",
    "        a,d=utils.make_Stationary(data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df_train, df_test = data[0:len(data)-test_size], data[len(data)-test_size:]\n",
    "        df_train_avg, df_test_avg = data_avg[0:len(data_avg)-test_size], data_avg[len(data_avg)-test_size:]\n",
    "        data_exg_train,data_exg_test= data_exg[0:len(data_exg)-test_size], data_exg[len(data_exg)-test_size:]\n",
    "        if d>0:\n",
    "            print(d)\n",
    "        #apply the model and get the result\n",
    "        Sarima(2,d,0,station_id)\n",
    "#         break\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time\n",
    "with open('sarimax_model_time.txt', 'w+') as f:\n",
    "        for item in model_times:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summaries=pd.concat(summaries)\n",
    "df_summaries.to_csv('sarimax_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
