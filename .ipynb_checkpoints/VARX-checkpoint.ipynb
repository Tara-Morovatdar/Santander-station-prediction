{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from pandas import read_csv\n",
    "import numpy as np # linear algebra\n",
    "import random as rd # generating random numbers\n",
    "import datetime # manipulating date formats\n",
    "import time\n",
    "import ast\n",
    "from math import sqrt\n",
    "# Viz\n",
    "import seaborn as sns # for prettier plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# TIME SERIES\n",
    "import scipy.stats as scs\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import seaborn; seaborn.set()\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from sklearn import preprocessing\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>operator_id</th>\n",
       "      <th>bikes</th>\n",
       "      <th>spaces</th>\n",
       "      <th>total_docks</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>37</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   operator_id  bikes  spaces  total_docks            timestamp\n",
       "0            1      9      10           19  2018-01-01 00:00:00\n",
       "1            2     21      16           37  2018-01-01 00:00:00\n",
       "2            3     12      20           32  2018-01-01 00:00:00\n",
       "3            4     16       7           23  2018-01-01 00:00:00\n",
       "4            5     10      17           27  2018-01-01 00:00:00"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"/bigdata/tara/ind_london_2018.csv\",index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ucl_id</th>\n",
       "      <th>operator_intid</th>\n",
       "      <th>operator_altid</th>\n",
       "      <th>operator_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>initial_bikes</th>\n",
       "      <th>initial_size</th>\n",
       "      <th>curr_bikes</th>\n",
       "      <th>curr_size</th>\n",
       "      <th>created_dt</th>\n",
       "      <th>updated_dt</th>\n",
       "      <th>neighbors_1</th>\n",
       "      <th>neighbors_2</th>\n",
       "      <th>station_id</th>\n",
       "      <th>change_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Malet Street, Bloomsbury</td>\n",
       "      <td>51.521681</td>\n",
       "      <td>-0.130432</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[364, 287]</td>\n",
       "      <td>[364, 287, 88, 19, 796]</td>\n",
       "      <td>12</td>\n",
       "      <td>38553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>Little Argyll Street, West End</td>\n",
       "      <td>51.514500</td>\n",
       "      <td>-0.141424</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[349, 159]</td>\n",
       "      <td>[159, 349, 313, 106, 141]</td>\n",
       "      <td>116</td>\n",
       "      <td>37611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>Leonard Circus , Shoreditch</td>\n",
       "      <td>51.524696</td>\n",
       "      <td>-0.084439</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[323]</td>\n",
       "      <td>[323, 73, 58, 3, 319]</td>\n",
       "      <td>32</td>\n",
       "      <td>40529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>Milroy Walk, South Bank</td>\n",
       "      <td>51.507244</td>\n",
       "      <td>-0.106238</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[839, 230, 240, 792]</td>\n",
       "      <td>[839, 240, 230, 792, 420]</td>\n",
       "      <td>195</td>\n",
       "      <td>36805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>Holborn Circus, Holborn</td>\n",
       "      <td>51.517950</td>\n",
       "      <td>-0.108657</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>2010-08-06 01:00:00</td>\n",
       "      <td>2020-02-14 17:38:02</td>\n",
       "      <td>[546, 67, 835]</td>\n",
       "      <td>[546, 67, 835, 84, 112]</td>\n",
       "      <td>66</td>\n",
       "      <td>49363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ucl_id  operator_intid  operator_altid                   operator_name  \\\n",
       "0      12              12              12        Malet Street, Bloomsbury   \n",
       "1     116             116             116  Little Argyll Street, West End   \n",
       "3      32              32              32     Leonard Circus , Shoreditch   \n",
       "6     195             195             195         Milroy Walk, South Bank   \n",
       "7      66              66              66         Holborn Circus, Holborn   \n",
       "\n",
       "         lat       lon  initial_bikes  initial_size  curr_bikes  curr_size  \\\n",
       "0  51.521681 -0.130432             25            40          26         49   \n",
       "1  51.514500 -0.141424             12            21           5         21   \n",
       "3  51.524696 -0.084439             17            21           9         43   \n",
       "6  51.507244 -0.106238              3            30          19         30   \n",
       "7  51.517950 -0.108657             39            39          26         40   \n",
       "\n",
       "            created_dt           updated_dt           neighbors_1  \\\n",
       "0  2010-08-06 01:00:00  2020-02-14 17:38:02            [364, 287]   \n",
       "1  2010-08-06 01:00:00  2020-02-14 17:38:02            [349, 159]   \n",
       "3  2010-08-06 01:00:00  2020-02-14 17:38:02                 [323]   \n",
       "6  2010-08-06 01:00:00  2020-02-14 17:38:02  [839, 230, 240, 792]   \n",
       "7  2010-08-06 01:00:00  2020-02-14 17:38:02        [546, 67, 835]   \n",
       "\n",
       "                 neighbors_2  station_id  change_count  \n",
       "0    [364, 287, 88, 19, 796]          12         38553  \n",
       "1  [159, 349, 313, 106, 141]         116         37611  \n",
       "3      [323, 73, 58, 3, 319]          32         40529  \n",
       "6  [839, 240, 230, 792, 420]         195         36805  \n",
       "7    [546, 67, 835, 84, 112]          66         49363  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_station=pd.read_csv('SampleData/sample_stations.csv',index_col=0)\n",
    "df_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>trip_count</th>\n",
       "      <th>temp</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>humidity</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>Clear</th>\n",
       "      <th>Clouds</th>\n",
       "      <th>...</th>\n",
       "      <th>Smoke</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Thunderstorm</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>trip_log</th>\n",
       "      <th>is_non_workday</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_rushhour</th>\n",
       "      <th>is_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>754</td>\n",
       "      <td>280.27</td>\n",
       "      <td>996</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.626718</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02 00:00:00</td>\n",
       "      <td>90</td>\n",
       "      <td>279.16</td>\n",
       "      <td>1009</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.510860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03 00:00:00</td>\n",
       "      <td>70</td>\n",
       "      <td>285.27</td>\n",
       "      <td>988</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>76</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.262680</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04 00:00:00</td>\n",
       "      <td>121</td>\n",
       "      <td>280.77</td>\n",
       "      <td>1001</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.804021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>94</td>\n",
       "      <td>278.85</td>\n",
       "      <td>993</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>81</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.553877</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  trip_count    temp  pressure  wind_speed  rain_1h  \\\n",
       "0  2018-01-01 00:00:00         754  280.27       996           9      0.0   \n",
       "1  2018-01-02 00:00:00          90  279.16      1009           4      0.0   \n",
       "2  2018-01-03 00:00:00          70  285.27       988          11      3.0   \n",
       "3  2018-01-04 00:00:00         121  280.77      1001           6      0.0   \n",
       "4  2018-01-05 00:00:00          94  278.85       993           3      0.3   \n",
       "\n",
       "   humidity  clouds_all  Clear  Clouds  ...  Smoke  Snow  Thunderstorm  \\\n",
       "0        70          68      0       1  ...      0     0             0   \n",
       "1        75          48      0       1  ...      0     0             0   \n",
       "2        76          92      0       0  ...      0     0             0   \n",
       "3        75          40      0       1  ...      0     0             0   \n",
       "4        81          92      0       0  ...      0     0             0   \n",
       "\n",
       "   is_weekend  is_holiday  trip_log  is_non_workday  hour  is_rushhour  is_day  \n",
       "0           0           1  6.626718               1     0            0       0  \n",
       "1           0           0  4.510860               0     0            0       0  \n",
       "2           0           0  4.262680               0     0            0       0  \n",
       "3           0           0  4.804021               0     0            0       0  \n",
       "4           0           0  4.553877               0     0            0       0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exg=pd.read_csv('SampleData/exg_df.csv',index_col=0)\n",
    "\n",
    "df_exg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data=pd.read_csv('SampleData/missing_data',index_col=0)\n",
    "useless_stations=[]\n",
    "for i in missing_data.columns:\n",
    "    \n",
    "    if int(i) in [1,3,5,7,8,10,12]:\n",
    "           \n",
    "        useless_stations.extend(list(missing_data.loc[missing_data[i] > (0.15*22320)].index))\n",
    "    elif int(i)==2:\n",
    "        useless_stations.extend(list(missing_data.loc[missing_data[i] > (0.15*20160)].index))\n",
    "\n",
    "    else:\n",
    "        useless_stations.extend(list(missing_data.loc[missing_data[i] > (0.15*21600)].index))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates=['2018-01-01','2018-02-01','2018-03-01','2018-04-01','2018-05-01','2018-06-01','2018-07-01','2018-08-01',\n",
    "             '2018-09-01','2018-10-01','2018-11-01','2018-12-01']\n",
    "end_dates=['2018-01-31','2018-02-28','2018-03-31','2018-04-30','2018-05-31','2018-06-30','2018-07-31','2018-08-31'\n",
    "           ,'2018-09-30','2018-10-31','2018-11-30','2018-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq=60 #the original freq of data is 2 minutes interval\n",
    "time_offset=60 #for how many minutes ahead we want to predict\n",
    "#forecast_steps=int(time_offset/freq) #number of predictions that we need to predict time_offset ahead\n",
    "test_days=7 #the number of days for test\n",
    "test_size=int(test_days*24*60/freq) \n",
    "forecast_steps=int(time_offset/freq)\n",
    "window_size=int(4*60/freq)\n",
    "Stations=list(df_station['ucl_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtest_size=int((test_days-1)*24*60/freq)\n",
    "xtest_size=164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_var_df(station_id):\n",
    "    neighbors=ast.literal_eval(df_station[df_station['ucl_id']==station_id]['neighbors_1'].values[0])\n",
    "    \n",
    "    var_df=data.copy()\n",
    "    #print(neighbors)\n",
    "    for n in neighbors:\n",
    "        if n not in useless_stations:\n",
    "\n",
    "            df_neighbor= df[df['operator_id']==n]\n",
    "            total_dock_neighbor=df[df['operator_id']==int(n)]['total_docks'].values[0]            \n",
    "            df_neighbor=utils.clean_df(df_neighbor,n,start_date,end_date,freq)\n",
    "            df_neighbor=utils.normalize(df_neighbor,total_dock_neighbor)\n",
    "            anomalies_n=utils.anomaly_detection(df_neighbor,freq)\n",
    "            df_neighbor=utils.anomaly_removal(anomalies_n,df_neighbor)\n",
    "           \n",
    "            var_df['spaces_'+str(n)] = pd.Series(df_neighbor['spaces_'+str(n)])\n",
    "    var_df=var_df.fillna(method='ffill')\n",
    "    var_df=var_df.fillna(method='bfill')\n",
    "    \n",
    "    return var_df\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "def grangers_causation_matrix(df_temp, variables, test='ssr_chi2test'):   \n",
    "    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    maxlag=12\n",
    "#     df_grangers = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "#     for c in df_grangers.columns:\n",
    "#         for r in df_grangers.index:\n",
    "    \n",
    "    for col in variables[1:]:\n",
    "        test_result = grangercausalitytests(df_temp[[variables[0], col]], maxlag=maxlag, verbose=False)\n",
    "        p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "        #if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "        min_p_value = np.min(p_values)\n",
    "        if min_p_value>0.05:\n",
    "            df_temp=df_temp.drop(col,axis=1)\n",
    "\n",
    "#     df_grangers.columns = [var + '_x' for var in variables]\n",
    "#     df_grangers.index = [var + '_y' for var in variables]\n",
    "    return df_temp\n",
    "        \n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_seasonality(temp_df,test_size):\n",
    "    df_temp=temp_df.copy()\n",
    "    avgs=df_temp[0:-test_size].groupby([df_temp[0:-test_size].index.dayofweek,df_temp[0:-test_size].index.time]).mean()\n",
    "    for name, column in (df_temp.iteritems()):\n",
    "        df_temp[name+'_avg'] = df_temp.index.map(lambda d: avgs.loc[(d.dayofweek,d.time()),name])\n",
    "        df_temp[name] = df_temp[name] - df_temp[name+'_avg']\n",
    "    \n",
    "        #df_temp=df_temp.drop(name+'_avg',axis=1)\n",
    "    n_cols=len(list(df_temp.columns))\n",
    "    df1 = df_temp.iloc[:, :int(n_cols/2)] #var_df\n",
    "    df2 = df_temp.iloc[:, int(n_cols/2):] #vard_df_avg\n",
    "    return df1,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_seasonality(df1,df2):\n",
    "    df_1=df1.copy()\n",
    "    df_2=df2.copy()\n",
    "    for name, column in (df_1.iteritems()):\n",
    "        \n",
    "        df_1[name] = df_1[name] + df_2[name.split('_forecast_')[0]+'_avg']\n",
    "    return df_1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varx(p_order,station_id,d):\n",
    "    global i\n",
    "    global summaries\n",
    "    global model_times\n",
    "    forecasts=[]\n",
    "    tests=[]\n",
    "    df_exg_history= data_exg_train.copy()\n",
    "    df_history=df_train.copy()\n",
    "    t0=test_size ##neded for reverse the diff\n",
    "\n",
    "    # walk-forward validation\n",
    "    for t in range(0,len(df_test),window_size):\n",
    "        predictions=np.zeros((0,len(df_train.columns)))\n",
    "        result={}\n",
    "        df_history.index = pd.DatetimeIndex(df_history.index.values,\n",
    "                                   freq=df_history.index.inferred_freq)\n",
    "        # fit model and predict\n",
    "#         print(df_history[-len(df_train):].head())\n",
    "#         print(df_exg_history[-len(df_train):].head())\n",
    "        start=time.time()\n",
    "        model  = VARMAX(df_history[-len(df_train):],exog=df_exg_history[-len(df_train):],order=(p_order, 0))\n",
    "        model_fitted = model.fit(disp=False)   \n",
    "\n",
    "        df_forecast = model_fitted.forecast(steps=window_size,exog=data_exg_test[t:t+window_size])  \n",
    "        #print(fc)\n",
    "        ### update indogenous and exogenous dataframes\n",
    "        df_history = df_history.append(df_test[t:t+window_size])\n",
    "        df_exg_history=df_exg_history.append(data_exg_test[t:t+window_size])\n",
    "        \n",
    "        ###create the forecast dataframe\n",
    "\n",
    "        df_forecast.columns = [ col+ '_forecast_'+str(d) for col in df_forecast.columns]\n",
    "\n",
    "        #######bring data back to normal scale \n",
    "        df_forecast=utils.invert_transformation_forecast(var_df_original, df_forecast,t0,diff_count=d)\n",
    "       \n",
    "        df_forecast= add_seasonality(df_forecast,df_test_avg[t:t+window_size])  \n",
    "        df_forecast=utils.inverse_normalize(df_forecast,total_dock)\n",
    "        \n",
    "        df_actual= utils.invert_transformation_test(var_df_original, df_test[t:t+window_size],t0,diff_count=d)\n",
    "\n",
    "        df_actual= add_seasonality(df_actual,df_test_avg[t:t+window_size])\n",
    "        df_actual=utils.inverse_normalize(df_actual,total_dock) \n",
    "        t0=test_size +window_size\n",
    "        df_forecast= utils.forecast_truncate(df_forecast,total_dock)\n",
    "        \n",
    "        ######################write to file\n",
    "        forecasts.extend(list(df_forecast.iloc[:,0].values))\n",
    "        tests.extend(list(df_actual.iloc[:,0].values))\n",
    "\n",
    "        if ((t+window_size)%(6*window_size) == 0):\n",
    "                result['day']=df_forecast.index[1].dayofweek\n",
    "                result['month']=df_forecast.index[1].month\n",
    "                result['station_id']=station_id\n",
    "    #             result['predict']=list(df_forecast.iloc[:,0].values)\n",
    "    #             result['test']=list(df_actual.iloc[:,0].values)\n",
    "                result['predict']=forecasts\n",
    "                result['test']=tests\n",
    "\n",
    "                with open('report/test/VARX/'+'varx_result_'+str(station_id)+'_'+str(i)+'.json', 'w+') as f:\n",
    "                #with open('report/SARIMA/'+str(freq)+'minutes/sarima_result_'+str(station_id)+'_'+str(i)+'.json', 'w+') as f:\n",
    "                    f.write(json.dumps(result))\n",
    "                i=i+1\n",
    "                forecasts=[]\n",
    "                tests=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarima(p,d,q,station_id):\n",
    "    global i\n",
    "    global summaries\n",
    "    global model_times\n",
    "    forecasts=[]\n",
    "    tests=[]\n",
    "    df_exg_history= data_exg_train.copy()\n",
    "    df_history=df_train.copy()\n",
    "    t0=test_size ##neded for reverse the diff\n",
    "    \n",
    "    t0=test_size ##neded for reverse the diff  \n",
    "    \n",
    "    # walk-forward validation\n",
    "    \n",
    "    \n",
    "    for t in range(0,len(df_test),window_size):\n",
    "        result={}\n",
    "        predictions = list()\n",
    "        #because of the warning for frequency\n",
    "        df_history.index = pd.DatetimeIndex(df_history.index.values,\n",
    "                                   freq=df_history.index.inferred_freq)\n",
    "        # fit model\n",
    "        start=time.time()\n",
    "        model = SARIMAX(df_history[-len(df_train):],order=(p,d,q),seasonal_order=(0,0,0,0),\n",
    "                        enforce_stationarity=False, enforce_invertibility=False,\n",
    "                        exog=df_exg_history[-len(df_train):], time_varying_regression=True, mle_regression=False)\n",
    "\n",
    "        model_fit = model.fit()\n",
    "        end=time.time()\n",
    "        model_times.append(end-start)\n",
    "\n",
    "        yhat = model_fit.forecast(steps=window_size,exog =data_exg_test[t:t+window_size])\n",
    "     \n",
    "        ### update indogenous and exogenous dataframes\n",
    "        df_history = df_history.append(df_test[t:t+window_size])\n",
    "        df_exg_history=df_exg_history.append(data_exg_test[t:t+window_size])\n",
    "        \n",
    "        #df_forecast = pd.DataFrame(yhat, index=df_test.index[t:t+window_size], columns=df_train.columns + '_forecast_'+str(d))\n",
    "        predictions.extend(yhat)\n",
    "\n",
    "        if t==xtest_size:\n",
    "            df_forecast = pd.DataFrame(predictions, index=data.index[-test_size+t:], columns=data.columns + '_forecast_'+str(d))\n",
    "        else:\n",
    "            df_forecast = pd.DataFrame(predictions, index=data.index[-test_size+t:-test_size+t+window_size], \n",
    "                                       columns=data.columns + '_forecast_'+str(d))\n",
    "        \n",
    "        #######bring forecast data back to normal scale \n",
    "       \n",
    "        df_forecast= add_seasonality(df_forecast,df_test_avg[t:t+window_size])  \n",
    "        #df_forecast=utils.invert_transformation_forecast(data_original, df_forecast,t0,diff_count=d)\n",
    "        df_forecast=utils.inverse_normalize(df_forecast,total_dock)\n",
    "        \n",
    "        ####bring test data back to normal scale \n",
    "        df_actual= add_seasonality(df_test[t:t+window_size],df_test_avg[t:t+window_size])\n",
    "       \n",
    "        #df_actual= utils.invert_transformation_test(data_original, df_test,t0,diff_count=d)\n",
    "        df_actual=utils.inverse_normalize(df_actual,total_dock) \n",
    "        \n",
    "       \n",
    " \n",
    "        t0=test_size +window_size\n",
    "        \n",
    "        df_forecast= utils.forecast_truncate(df_forecast,total_dock)\n",
    "        ######################write to file\n",
    "        forecasts.extend(list(df_forecast.iloc[:,0].values))\n",
    "        tests.extend(list(df_actual.iloc[:,0].values))\n",
    "\n",
    "        if ((t+window_size)%(6*window_size) == 0):\n",
    "                result['day']=df_forecast.index[1].dayofweek\n",
    "                result['month']=df_forecast.index[1].month\n",
    "                result['station_id']=station_id\n",
    "    #             result['predict']=list(df_forecast.iloc[:,0].values)\n",
    "    #             result['test']=list(df_actual.iloc[:,0].values)\n",
    "                result['predict']=forecasts\n",
    "                result['test']=tests\n",
    "\n",
    "                with open('report/test/VARX/'+'varx_result_'+str(station_id)+'_'+str(i)+'.json', 'w+') as f:\n",
    "                #with open('report/SARIMA/'+str(freq)+'minutes/sarima_result_'+str(station_id)+'_'+str(i)+'.json', 'w+') as f:\n",
    "                    f.write(json.dumps(result))\n",
    "                i=i+1\n",
    "                forecasts=[]\n",
    "                tests=[]\n",
    "#         break\n",
    "    #return df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-01\n",
      "32\n",
      "130\n",
      "278\n",
      "73\n",
      "213\n",
      "732\n",
      "2018-07-01\n",
      "32\n",
      "194\n",
      "130\n",
      "278\n",
      "213\n",
      "732\n",
      "2018-08-01\n",
      "32\n",
      "130\n",
      "278\n",
      "732\n",
      "2018-09-01\n",
      "32\n",
      "130\n",
      "278\n",
      "732\n",
      "2018-10-01\n",
      "32\n",
      "130\n",
      "278\n",
      "73\n",
      "732\n",
      "2018-11-01\n",
      "32\n",
      "130\n",
      "107\n",
      "213\n",
      "732\n",
      "2018-12-01\n",
      "32\n",
      "130\n",
      "278\n",
      "73\n",
      "246\n",
      "107\n",
      "213\n",
      "732\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "summaries=[]\n",
    "model_times=list()\n",
    "p_order_times=[]\n",
    "p_orders=[]\n",
    "i=1308\n",
    "for t in range(5,12):\n",
    "  \n",
    "    start_date=start_dates[t]\n",
    "    end_date=end_dates[t]        \n",
    "    print(start_date)\n",
    "    data_exg_original=utils.clean_exg_df(df_exg,start_date,end_date,freq)\n",
    "\n",
    "\n",
    "    for station_id in Stations:        \n",
    "        \n",
    "        \n",
    "        total_dock=df[df['operator_id']==int(station_id)]['total_docks'].values[0]\n",
    "        data=df[df['operator_id']==station_id]\n",
    "        \n",
    "        #data prepration     \n",
    "        data=utils.clean_df(data,station_id,start_date,end_date,freq)\n",
    "        data=utils.normalize(data,total_dock)\n",
    " \n",
    "        anomalies=utils.anomaly_detection(data,freq)\n",
    "        data=utils.anomaly_removal(anomalies,data)\n",
    "        \n",
    "        var_df=create_var_df(station_id)\n",
    "\n",
    "        var_df_original=grangers_causation_matrix(var_df, variables = list(var_df.columns))\n",
    "        #remove trend if there is any\n",
    "        var_df_original,var_df_avg=remove_seasonality(var_df_original,test_size)\n",
    "        var_df,d=utils.make_Stationary(var_df_original)\n",
    "        \n",
    "        data_exg=data_exg_original[data_exg_original.index.isin(var_df.index)]\n",
    "        df_train, df_test = var_df[0:len(data)-test_size], var_df[len(data)-test_size:]\n",
    "        df_train_avg, df_test_avg = var_df_avg[0:len(data)-test_size], var_df_avg[len(data)-test_size:]\n",
    "        data_exg_train,data_exg_test= data_exg[0:len(data)-test_size], data_exg[len(data)-test_size:]\n",
    "\n",
    "        if len(list(var_df.columns))>1:                   \n",
    "            start=time.time()\n",
    "            model = VAR(df_train)\n",
    "            x=model.select_order(maxlags=12)\n",
    "            p=x.selected_orders['bic']\n",
    "            end=time.time()\n",
    "            p_order_times.append(end-start)\n",
    "            if (p>0):\n",
    "                varx(p,station_id,d)\n",
    "            else:\n",
    "                p=2\n",
    "                varx(p,station_id,d)\n",
    "        else:\n",
    "            print(station_id)\n",
    "            \n",
    "            Sarima(2,d,0,station_id)\n",
    "        p_orders.append(p)\n",
    "\n",
    "\n",
    "\n",
    "# end=time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summaries=pd.concat(summaries)\n",
    "# df_summaries.to_csv('varx_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('varx_orders.txt', 'w+') as f:\n",
    "    for item in p_orders:\n",
    "        f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('varx_model_time.txt', 'w+') as f:\n",
    "        for item in model_times:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
